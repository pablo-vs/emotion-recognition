{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'processed_videos/'\n",
    "VIDEO_DIR = 'videos/'\n",
    "SCORE_DIR = 'scores/'\n",
    "MODEL_DIR = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        #self.hidden = torch.nn.Linear(1000,100)\n",
    "        self.classifiers = [torch.nn.Linear(1000,1) for _ in range(4)]\n",
    "        self.classifiers = torch.nn.ModuleList(self.classifiers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        #x = sigmoid(self.hidden(self.backbone(x)))\n",
    "        x = self.backbone(x)\n",
    "        x1,x2,x3,x4 = [sigmoid(f(x)) for f in self.classifiers]\n",
    "        x = torch.cat((x1,x2,x3,x4),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "def read_filenames(filename):\n",
    "    \"Given the name of the information file, reads all video and label filenames.\"\n",
    "    video_files,score_files = [],[]\n",
    "    with open(DATA_DIR + filename) as f:\n",
    "        for line in f:\n",
    "            vf,sf = line.strip('\\n').split(',')\n",
    "            video_files.append(vf)\n",
    "            score_files.append(sf)\n",
    "    return video_files,score_files\n",
    "\n",
    "# Generate labels\n",
    "def generate_video_labels(vid_filename,score_filename, drop=20):\n",
    "    \"\"\"Given a video filename and its associated score filename, \n",
    "    returns the frame ids and scores downsampled by the drop factor.\"\"\"\n",
    "    cap = cv2.VideoCapture(DATA_DIR+VIDEO_DIR+vid_filename)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))-1\n",
    "    frame_labels = [vid_filename+'_frame_'+str(i+1) for i in range(0,length,drop)]\n",
    "    frame_scores = np.load(DATA_DIR+SCORE_DIR+score_filename)[:-1:drop,:]\n",
    "    return frame_labels,frame_scores\n",
    "\n",
    "# Generate dataset information\n",
    "def generate_dataset_info(filename, drop=20):\n",
    "    \"\"\"Creates identifiers and scores for from the information file \n",
    "    downsapling videos by the drop factor.\"\"\"\n",
    "    video_files,score_files = read_filenames(filename)\n",
    "    # Generate identifiers for each frame in the video files\n",
    "    frame_ids,frame_scores = [],[]\n",
    "    for v_fname,s_fname in zip(video_files,score_files):\n",
    "        vf_id,vf_scores = generate_video_labels(v_fname,s_fname, drop)\n",
    "        frame_ids += vf_id\n",
    "        frame_scores += list(vf_scores)\n",
    "        print(\"Video:\",v_fname,\" - \",(len(vf_id),len(vf_scores)))\n",
    "    return frame_ids, frame_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET LOADER FOR TORCH\n",
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        \"\"\"Initializes the Dataset object with the list of frame identifiers \n",
    "        and the label dictionary.\"\"\"\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        # Normalization params for torchvision models\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        # Preprocessing layer\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "  def __len__(self):\n",
    "        \"\"\"Returns the total number of frames in the Dataset.\"\"\"\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        \"\"\"Generates a processed frame with its associated scores.\"\"\"\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        # Extract video name from ID\n",
    "        v_name,frame = ID.split('.')\n",
    "        v_name += '.webm'\n",
    "        frame = int(frame.split('_')[-1])\n",
    "        # Load frame and label\n",
    "        cap = cv2.VideoCapture(DATA_DIR+VIDEO_DIR+v_name)\n",
    "        cap.set(1,frame)\n",
    "        _,frame = cap.read()\n",
    "        cap.release()\n",
    "        frame=Image.fromarray(frame,'RGB')\n",
    "        label = self.labels[ID]\n",
    "        return self.preprocess(frame), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "def test_model(model, generator, use_cuda=False):\n",
    "    \"\"\"Returns the results of the model on the given generator for each dimension as a dictionary, \n",
    "    as well as the predictions, ground truth and average inference time.\"\"\"\n",
    "    model.eval()\n",
    "    predicted,labels,inf_times = [],[],[]\n",
    "    it = 1\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in generator:\n",
    "            # Transfer to GPU and model computations\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            print('Testing batch %i/%i' % (it, len(generator)))\n",
    "            if use_cuda:\n",
    "                start = torch.cuda.Event(enable_timing=True)\n",
    "                end = torch.cuda.Event(enable_timing=True)\n",
    "                # Record inference time\n",
    "                start.record()\n",
    "                out = model(local_batch)\n",
    "                end.record()\n",
    "                # Sync\n",
    "                torch.cuda.synchronize()\n",
    "                inf_times.append(start.elapsed_time(end))\n",
    "            else:\n",
    "                # Record inference time\n",
    "                start = time.time()\n",
    "                out = model(local_batch)\n",
    "                end = time.time()\n",
    "                inf_times.append(end-start)\n",
    "            predicted.append(out)\n",
    "            labels.append(local_labels)\n",
    "            it += 1\n",
    "    pred,true=[],[]\n",
    "    pred_np,true_np = [],[]\n",
    "    if use_cuda:\n",
    "        pred_np = [x.cpu().numpy() for x in predicted][:-1]\n",
    "        true_np = [x.cpu().numpy() for x in labels][:-1]\n",
    "    else:\n",
    "        pred_np = predicted[:-1]\n",
    "        true_np = labels[:-1]\n",
    "    pred = np.stack(pred_np, axis=0).reshape(-1, 4)\n",
    "    true = np.stack(true_np, axis=0).reshape(-1, 4)\n",
    "    cols = ['Af','En','Ec','Le']\n",
    "    scores_by_col = dict()\n",
    "    for col in [0,1,2,3]:\n",
    "        scores_by_col[cols[col]] = {\"MSE\": mean_squared_error(true[:,col], pred[:,col]),\n",
    "        \"MAE\": mean_absolute_error(true[:,col], pred[:,col]),\n",
    "        \"MAPE\": mean_absolute_percentage_error(true[:,col], pred[:,col]),\n",
    "        \"R2\":r2_score(true[:,col], pred[:,col])}\n",
    "    return pred,true,scores_by_col,sum(inf_times)/len(inf_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: track1_11.webm  -  (571, 571)\n",
      "Video: track1_1.webm  -  (581, 581)\n",
      "Video: track1_4.webm  -  (585, 585)\n",
      "Video: track1_5.webm  -  (486, 486)\n",
      "Video: track1_7.webm  -  (577, 577)\n",
      "Video: track2_11.webm  -  (794, 794)\n",
      "Video: track2_1.webm  -  (803, 803)\n",
      "Video: track2_4.webm  -  (801, 801)\n",
      "Video: track2_5.webm  -  (671, 671)\n",
      "Video: track2_7.webm  -  (800, 800)\n",
      "Video: track3_11.webm  -  (424, 424)\n",
      "Video: track3_1.webm  -  (432, 432)\n",
      "Video: track3_4.webm  -  (440, 440)\n",
      "Video: track3_5.webm  -  (359, 359)\n",
      "Video: track3_7.webm  -  (426, 426)\n",
      "Video: track1_2.webm  -  (574, 574)\n",
      "Video: track2_2.webm  -  (781, 781)\n",
      "Video: track3_2.webm  -  (426, 426)\n",
      "Video: track1_2.webm  -  (574, 574)\n",
      "Video: track2_2.webm  -  (781, 781)\n",
      "Video: track3_2.webm  -  (426, 426)\n",
      "Video: track1_6.webm  -  (592, 592)\n",
      "Video: track2_6.webm  -  (799, 799)\n",
      "Video: track3_6.webm  -  (409, 409)\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset info (train,test,validation)\n",
    "train_ids, train_scores = generate_dataset_info('files_fulltrain.txt', drop=20)\n",
    "#train_ids, train_scores = generate_dataset_info('files_train.txt', drop=20)\n",
    "val_ids, val_scores = generate_dataset_info('files_validation.txt', drop=20)\n",
    "test_ids, test_scores = generate_dataset_info('files_test.txt',drop=20)\n",
    "\n",
    "# Partition initialization\n",
    "partition = dict()\n",
    "labels = dict()\n",
    "partition['train'] = train_ids\n",
    "partition['validation'] = val_ids\n",
    "partition['test'] = test_ids\n",
    "for x,y in zip(train_ids,train_scores):\n",
    "    labels[x] = y\n",
    "for x,y in zip(val_ids,val_scores):\n",
    "    labels[x] = y\n",
    "for x,y in zip(test_ids,test_scores):\n",
    "    labels[x] = y\n",
    "\n",
    "# Initializing data generators and datasets\n",
    "params = {'batch_size': 256,\n",
    "          'shuffle': True,\n",
    "          'pin_memory': True,\n",
    "          'num_workers': 0} \n",
    "\n",
    "train_dataset = Dataset(partition['train'], labels)\n",
    "val_dataset = Dataset(val_ids,labels)\n",
    "test_dataset = Dataset(test_ids,labels)\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_dataset, **params)\n",
    "val_generator = torch.utils.data.DataLoader(val_dataset, **params)\n",
    "test_generator = torch.utils.data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Enabling CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"USE_CUDA:\",use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation parameters\n",
    "max_epochs = 1\n",
    "model = Net()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.L1Loss(reduce=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA: True\n",
      "Training batch 1/42\n",
      "Training batch 2/42\n",
      "Training batch 3/42\n",
      "Training batch 4/42\n",
      "Training batch 5/42\n",
      "Training batch 6/42\n",
      "Training batch 7/42\n",
      "Training batch 8/42\n",
      "Training batch 9/42\n",
      "Training batch 10/42\n",
      "Training batch 11/42\n",
      "Training batch 12/42\n",
      "Training batch 13/42\n",
      "Training batch 14/42\n",
      "Training batch 15/42\n",
      "<<Training>> [1,    17] loss: 0.152\n",
      "Training batch 16/42\n",
      "Training batch 17/42\n",
      "Training batch 18/42\n",
      "Training batch 19/42\n",
      "Training batch 20/42\n",
      "Training batch 21/42\n",
      "Training batch 22/42\n",
      "Training batch 23/42\n",
      "Training batch 24/42\n",
      "Training batch 25/42\n",
      "Training batch 26/42\n",
      "Training batch 27/42\n",
      "Training batch 28/42\n",
      "Training batch 29/42\n",
      "Training batch 30/42\n",
      "Training batch 31/42\n",
      "<<Training>> [1,    33] loss: 0.138\n",
      "Training batch 32/42\n",
      "Training batch 33/42\n",
      "Training batch 34/42\n",
      "Training batch 35/42\n",
      "Training batch 36/42\n",
      "Training batch 37/42\n",
      "Training batch 38/42\n",
      "Training batch 39/42\n",
      "Training batch 40/42\n",
      "Training batch 41/42\n",
      "Training batch 42/42\n",
      "<<Training>> Accumulated loss: 5.992 Average loss: 0.143\n",
      "Training loss (epochwise): [0.14267540723085403]\n",
      "Validation loss (epochwise): []\n"
     ]
    }
   ],
   "source": [
    "N_AVG = 16\n",
    "\n",
    "train_history,val_history = [],[]\n",
    "# Training and validating the network\n",
    "for epoch in range(max_epochs):\n",
    "    ############################ Training #############################\n",
    "    model.train()\n",
    "    total_loss,running_loss,it = 0.0,0.0,1\n",
    "    for local_batch, local_labels in train_generator:\n",
    "        # Transfer to GPU and model computations\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)   \n",
    "        print('Training batch %i/%i' % (it, len(train_generator)))\n",
    "        optimizer.zero_grad()\n",
    "        out = model(local_batch)\n",
    "        loss = criterion(out, local_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += float(loss)\n",
    "        total_loss += float(loss)\n",
    "        it += 1\n",
    "        if it % N_AVG == 0:\n",
    "            print('<<Training>> [%d, %5d] loss: %.3f' % (epoch + 1, it + 1, running_loss / N_AVG))\n",
    "            running_loss = 0.0\n",
    "    print(\"<<Training>> Accumulated loss: %.3f Average loss: %.3f\" % (total_loss, total_loss/len(train_generator)))\n",
    "    train_history.append(total_loss/len(train_generator))\n",
    "    ################################## Validation ####################\n",
    "    continue\n",
    "    model.eval()\n",
    "    total_loss,running_loss,it = 0.0,0.0,0\n",
    "    it = 1\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in val_generator:\n",
    "            # Transfer to GPU and model computations\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            print('Validation batch %i/%i' % (it, len(val_generator)))\n",
    "            out = model(local_batch)\n",
    "            loss = criterion(out, local_labels)\n",
    "            running_loss += float(loss)\n",
    "            total_loss += float(loss)\n",
    "            it += 1\n",
    "            if it % N_AVG == 0:\n",
    "                print('<<Validation>> [%d, %5d] loss: %.3f' % (epoch + 1, it + 1, running_loss / N_AVG))\n",
    "                running_loss = 0.0\n",
    "    print(\"<<Validation>> Accumulated loss: %.3f Average loss: %.3f\" % (total_loss, total_loss/len(val_generator)))\n",
    "    val_history.append(total_loss/len(val_generator))\n",
    "print('Training loss (epochwise):', train_history)\n",
    "print('Validation loss (epochwise):', val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"train loss\")\n",
    "plt.plot(val_history, label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.xlim(0,max_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving\n",
    "filename = 'weights_9'\n",
    "if use_cuda:\n",
    "    torch.save(model.cpu().state_dict(), MODEL_DIR + filename)\n",
    "else:\n",
    "    torch.save(model.state_dict(), MODEL_DIR + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading\n",
    "if use_cuda:\n",
    "    model.load_state_dict(torch.load(\"models/weights_6\"))\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"models/model_weights\",map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "pred_test,gt_test,test_scores,inf_time = test_model(model,test_generator, use_cuda=True)\n",
    "print(test_scores)\n",
    "print(\"Average inference time:\", inf_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
